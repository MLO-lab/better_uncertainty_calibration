{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0dedc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/fm03-mb03/Repositories/proper_calibration_errors\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By the way, this is Julia not Python lol\n",
    "\n",
    "# code adapted from Wiedman et al 2021\n",
    "# https://github.com/devmotion/Calibration_ICLR2021\n",
    "\n",
    "using Base.Filesystem\n",
    "\n",
    "using Arrow\n",
    "using CairoMakie\n",
    "using CalibrationErrors\n",
    "using CalibrationErrorsDistributions\n",
    "using CalibrationTests\n",
    "using CSV\n",
    "using DataFrames\n",
    "using Distributions\n",
    "using Flux\n",
    "using Optim\n",
    "using Random\n",
    "using Statistics\n",
    "using ProgressLogging\n",
    "using Query\n",
    "\n",
    "using Logging: with_logger\n",
    "using TerminalLoggers: TerminalLogger\n",
    "using ColorSchemes: Dark2_8\n",
    "\n",
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97255c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split_input_target (generic function with 2 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data downloaded from http://archive.ics.uci.edu/ml/datasets/Residential+Building+Data+Set\n",
    "data = DataFrame(CSV.read(\"data/Residential-Building-Data-Set.csv\", DataFrame; delim=';', decimal=',', header=true))\n",
    "data = select(data, Not(\"V-9\"))\n",
    "\n",
    "data_id = \"ResBuild\"\n",
    "\n",
    "function random_split(df, n)\n",
    "    n_0 = size(df)[1]\n",
    "    indices = randperm(n_0)\n",
    "    train_i = indices[1:n]\n",
    "    eval_i = indices[(n+1):n_0]\n",
    "    return (df[train_i, :], df[eval_i, :])\n",
    "end\n",
    "\n",
    "function split_input_target(df, target, eps=0)\n",
    "    # potentially add gaussian noise via 'eps'\n",
    "    ys = select(df, target)\n",
    "    xs = select(df, Not(target))\n",
    "    dist = Normal(0, 1)\n",
    "    n = size(df)[1]\n",
    "    d = size(df)[2] - 1\n",
    "    n_samples = n * d\n",
    "    noise = rand(dist, n_samples) * eps\n",
    "    pert_xs = Matrix(xs) + reshape(noise, n, d)\n",
    "    \n",
    "    return (transpose(pert_xs), vec(Matrix(ys)))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bad90b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(100)\n",
    "\n",
    "n_models = 5\n",
    "\n",
    "df_train, df_eval = random_split(data, 100);\n",
    "df_val, df_test = random_split(df_eval, 100);\n",
    "\n",
    "target = \"V-10\"\n",
    "train_data = split_input_target(df_train, target);\n",
    "val_data = split_input_target(df_val, target);\n",
    "test_data = split_input_target(df_test, target);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e09f3d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var_se_dist (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function nn_model()\n",
    "    ## initial parameters\n",
    "    f = Chain(\n",
    "        Dense(107 => 200, relu; init=Flux.glorot_uniform),\n",
    "        Dense(200 => 2; init=Flux.glorot_uniform),\n",
    "    )\n",
    "    # due to me being stupid in julia, the variance output is in the log space\n",
    "    # and will be transformed in each loss later\n",
    "    return f\n",
    "end\n",
    "\n",
    "# we called this Dawid-Sebastiani-Score (DSS) in the paper\n",
    "function pmcc(ps, ys)\n",
    "    vars = abs.(ps[2,:])\n",
    "    scores = (ps[1,:] .- ys).^2 ./ vars .+ log.(vars)\n",
    "    return mean(scores)\n",
    "end\n",
    "\n",
    "function skce_biased(ps, ys)\n",
    "    kernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()\n",
    "    estimator = BiasedSKCE(kernel)\n",
    "    n = size(ps, 2)\n",
    "    predictions = [Normal(ps[1, i], sqrt(abs(ps[2, i]))) for i in 1:n]\n",
    "    return calibrationerror(estimator, vec(predictions), ys)\n",
    "end\n",
    "\n",
    "function skce_unbiased(ps, ys)\n",
    "    kernel = WassersteinExponentialKernel() ⊗ SqExponentialKernel()\n",
    "    estimator = UnbiasedSKCE(kernel)\n",
    "    n = size(ps, 2)\n",
    "    predictions = [Normal(ps[1, i], sqrt(abs(ps[2, i]))) for i in 1:n]\n",
    "    return calibrationerror(estimator, vec(predictions), ys)\n",
    "end\n",
    "\n",
    "function mse(ps, ys)\n",
    "    scores = (ps[1,:] .- ys).^2\n",
    "    return mean(scores)\n",
    "end\n",
    "\n",
    "function mean_var(ps)\n",
    "    vars = abs.(ps[2,:])\n",
    "    return mean(vars)\n",
    "end\n",
    "\n",
    "function var_se_dist(ps, ys)\n",
    "    # how much does the predicted var with the squared error of the mean prediction correspond?\n",
    "    # > 1, too pessimistic\n",
    "    # < 1, too optimistic\n",
    "    # cheers to reviewer #3 for asking!\n",
    "    vars = abs.(ps[2,:])\n",
    "    squared_errors = (ps[1,:] .- ys).^2\n",
    "    ratios = squared_errors ./ vars\n",
    "    return mean(ratios), var(ratios)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d809226e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recal (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 1e-10\n",
    "\n",
    "function recal(f, val_xs, val_ys)\n",
    "    preds = f(val_xs)\n",
    "    \n",
    "    function helper(w)\n",
    "        cals = transpose(hcat(preds[1, :], abs.(preds[2, :]) .* w[1] .+ w[2]))\n",
    "        return pmcc(cals, val_ys)\n",
    "    end\n",
    "\n",
    "    lower = [epsilon, epsilon]\n",
    "    upper = [Inf, Inf]\n",
    "    res = optimize(helper, lower, upper, [1.0, 2*epsilon], Fminbox(LBFGS()))\n",
    "    w = Optim.minimizer(res)\n",
    "\n",
    "    return x -> transpose(hcat(x[1, :], abs.(x[2, :]) .* w[1] .+ w[2]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ed4196d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function for distribution shift\n",
    "function dist_shift_helper(file, test_xs, ys, models)\n",
    "    epsilons = (0:5) .* 2000000\n",
    "\n",
    "    @progress name = \"iterations\" for (i, eps) in enumerate(epsilons)\n",
    "\n",
    "        ## compute gradients\n",
    "        gradients = gradient(Flux.params(test_xs)) do\n",
    "            return mean_var(models[1](test_xs))\n",
    "        end\n",
    "        # FGSM (without sign; makes more sense for tabular data)\n",
    "        pert_xs = test_xs + gradients[test_xs] * eps\n",
    "\n",
    "        # FGSM (with sign)\n",
    "        #pert_xs = test_xs + sign.(gradients[test_xs]) * eps\n",
    "\n",
    "        preds = models[1](pert_xs)\n",
    "\n",
    "        ## mean squared error\n",
    "        mse_v = mse(preds, ys)\n",
    "        println(file, eps, \",MSE,\", mse_v, \",uncal\")            \n",
    "\n",
    "        ## mean predicted var\n",
    "        var = mean_var(preds)\n",
    "        println(file, eps, \",Avg Var,\", var, \",uncal\")\n",
    "\n",
    "        ## ratio of predicted var and squared error of mean\n",
    "        avg_ratio, var_ratio = var_se_dist(preds, ys)\n",
    "        println(file, eps, \",Var SE Dist,\", avg_ratio, \",uncal\")\n",
    "        println(file, eps, \",Var SE Dist Var,\", var_ratio, \",uncal\")\n",
    "\n",
    "        preds_cal = models[2](models[1](pert_xs))\n",
    "\n",
    "        ## mean squared error\n",
    "        mse_v = mse(preds_cal, ys)\n",
    "        println(file, eps, \",MSE,\", mse_v, \",cal\")            \n",
    "\n",
    "        ## mean predicted var\n",
    "        var = mean_var(preds_cal)\n",
    "        println(file, eps, \",Avg Var,\", var, \",cal\")\n",
    "\n",
    "        ## ratio of predicted var and squared error of mean\n",
    "        avg_ratio, var_ratio = var_se_dist(preds_cal, ys)\n",
    "        println(file, eps, \",Var SE Dist,\", avg_ratio, \",cal\")\n",
    "        println(file, eps, \",Var SE Dist Var,\", var_ratio, \",cal\")\n",
    "    end\n",
    "end\n",
    "\n",
    "function evaluate_helper(file, i, preds, ys)\n",
    "\n",
    "    ## mean squared error\n",
    "    mse_v = mse(preds, ys)\n",
    "    println(file, i - 1, \",MSE,\", mse_v)            \n",
    "\n",
    "    ## mean-variance score\n",
    "    pmcc_v = pmcc(preds, ys)\n",
    "    println(file, i - 1, \",PMCC,\", pmcc_v)\n",
    "\n",
    "    ## unbiased estimator of SKCE\n",
    "    skce = skce_unbiased(preds, ys)\n",
    "    println(file, i - 1, \",SKCE (unbiased),\", skce)\n",
    "\n",
    "    ## biased estimator of SKCE\n",
    "    skce = skce_biased(preds, ys)\n",
    "    println(file, i - 1, \",SKCE (biased),\", skce)\n",
    "\n",
    "    ## mean predicted var\n",
    "    var = mean_var(preds)\n",
    "    println(file, i - 1, \",Avg Var,\", var)\n",
    "\n",
    "    ## ratio of predicted var and squared error of mean\n",
    "    avg_ratio, var_ratio = var_se_dist(preds, ys)\n",
    "    println(file, i - 1, \",Var SE Dist,\", avg_ratio)\n",
    "    println(file, i - 1, \",Var SE Dist Var,\", var_ratio)\n",
    "end\n",
    "\n",
    "# ## Training\n",
    "#\n",
    "# We use a maximum likelihood approach and train the parameters $\\theta$ of the model\n",
    "# for 4000 iterations by minimizing the DSS on the training data set\n",
    "# using ADAM.\n",
    "#\n",
    "# We train 5 models and compute the predicted distributions on the training and test data sets\n",
    "# in each iteration step.\n",
    "#\n",
    "# The initial values of the weight matrices of the neural networks are sampled from the\n",
    "# [uniform Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
    "# and the offset vectors are initialized with zeros. The model parameters are learnt by\n",
    "# iteratively minimizing the DSS on the training data set.\n",
    "# The parameters of the neural networks are trained by gradient descent with the\n",
    "# [Adam optimization algorithm](https://arxiv.org/pdf/1412.6980.pdf) (default\n",
    "# settings in [Flux.jl](https://github.com/FluxML/Flux.jl)).\n",
    "\n",
    "\n",
    "function train(file_shift, id, (train_xs, train_ys), (val_xs, val_ys), (test_xs, test_ys), niters = 2100)\n",
    "    \n",
    "    out_train = joinpath(\"data\", data_id, \"statistics_id=$(id)_dataset=train.csv\")\n",
    "    out_test = joinpath(\"data\", data_id, \"statistics_id=$(id)_dataset=test.csv\")\n",
    "    out_test_rc = joinpath(\"data\", data_id, \"statistics_id=$(id)_dataset=test_rc.csv\")\n",
    "    \n",
    "    for file in [out_train, out_test, out_test_rc]\n",
    "        mkpath(dirname(file))\n",
    "    end\n",
    "\n",
    "    open(out_train, \"w\") do file_train; open(out_test, \"w\") do file_test; open(out_test_rc, \"w\") do file_test_rc\n",
    "\n",
    "        println(file_train, \"iteration,statistic,estimate\")\n",
    "        println(file_test, \"iteration,statistic,estimate\")\n",
    "        println(file_test_rc, \"iteration,statistic,estimate\")\n",
    "\n",
    "        ## compute the predictions of the initial neural network\n",
    "        f = nn_model()\n",
    "\n",
    "        ## train with ADAM\n",
    "        params = Flux.Params(Flux.params(f))\n",
    "        opt = ADAM()\n",
    "        @progress name = \"training (id = $id)\" for i in 1:(niters + 1)\n",
    "            ## compute gradients\n",
    "            gradients = gradient(params) do\n",
    "                return pmcc(f(train_xs), train_ys)\n",
    "            end\n",
    "\n",
    "            ## update the parameters\n",
    "            Flux.Optimise.update!(opt, params, gradients)\n",
    "\n",
    "            g = recal(f, val_xs, val_ys)\n",
    "            evaluate_helper(file_train, i, f(train_xs), train_ys)\n",
    "            evaluate_helper(file_test, i, f(test_xs), test_ys)\n",
    "            evaluate_helper(file_test_rc, i, g(f(test_xs)), test_ys)\n",
    "\n",
    "#             if i == 2001\n",
    "#                 models = (f, g)\n",
    "#                 dist_shift_helper(file_shift, test_xs, test_ys, models)\n",
    "#             end\n",
    "        end\n",
    "    end; end; end    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0b8323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: training NN model: run 1\n",
      "└ @ Main In[16]:9\n",
      "┌ Info: training NN model: run 2\n",
      "└ @ Main In[16]:9\n",
      "┌ Info: training NN model: run 3\n",
      "└ @ Main In[16]:9\n",
      "┌ Info: training NN model: run 4\n",
      "└ @ Main In[16]:9\n",
      "┌ Info: training NN model: run 5\n",
      "└ @ Main In[16]:9\n"
     ]
    }
   ],
   "source": [
    "out_shift = \"data/dist_shift_uci_fgm.csv\"\n",
    "mkpath(dirname(out_shift))\n",
    "\n",
    "open(out_shift, \"w\") do file_shift\n",
    "    println(file_shift, \"epsilon,statistic,estimate,model\")\n",
    "\n",
    "    Random.seed!(100)\n",
    "    for (id, seed) in enumerate(rand(UInt, n_models))\n",
    "        @info \"training NN model: run $id\"\n",
    "        Random.seed!(seed)\n",
    "        train(file_shift, id, train_data, val_data, test_data)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ec429b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME TESTS FOR Dawid-Sebastiani Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9283d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 0\n",
    "var = 1e7\n",
    "\n",
    "d = Normal(mu, var^.5)\n",
    "n = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8f2ec79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_func (generic function with 2 methods)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _func(n)\n",
    "    y = rand(d, n);\n",
    "    x = transpose(hcat(zero(y) .+ mu, zero(y) .+ var))\n",
    "    best = pmcc(x, y)\n",
    "    #println(best)\n",
    "\n",
    "    x = transpose(hcat(zero(y) .+ mu, zero(y) .+ var ./ 0.8))\n",
    "    other = pmcc(x, y)\n",
    "    #println(other)\n",
    "\n",
    "    return best < other\n",
    "end    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "564b43d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9928"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean([_func(1000) for _ in 1:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a1ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
